{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9TyyUnHrjmHO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import h5py\n","import cv2\n","import os\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, TensorDataset, random_split\n","import requests\n","import torch.optim as optim\n","import pandas as pd\n","from tqdm import tqdm"]},{"cell_type":"code","source":["label = pd.read_csv('san_diego_2024-01-01_to_2024-03-09.csv')"],"metadata":{"id":"OMpVK0t8Ju32"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = label['temp'][:48]\n","y = torch.tensor(y)"],"metadata":{"id":"g5jq82WxKAZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to download image from URL and save to folder\n","def download_image(url, folder):\n","    # Get the filename from the URL\n","    filename = url.split('/')[-1]\n","\n","    # Create the folder if it doesn't exist\n","    os.makedirs(folder, exist_ok=True)\n","\n","    # Download the image and save it to the folder\n","    with open(os.path.join(folder, filename), 'wb') as f:\n","        response = requests.get(url)\n","        f.write(response.content)\n","    print(f\"Downloaded {filename}\")\n","\n","# Read the text file containing image URLs\n","file_path = 'L2_LST_images.txt'  # Replace with the path to your text file\n","with open(file_path, 'r') as file:\n","    urls = file.readlines()\n","\n","# Remove whitespace characters like `\\n` at the end of each line\n","urls = [url.strip() for url in urls]\n","\n","# Specify the folder to save the downloaded images\n","output_folder = 'L2_LST_images'\n","\n","# Download images and save them to the folder\n","for url in urls:\n","    download_image(url, output_folder)\n","\n","print(\"All images downloaded and saved to\", output_folder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMVEgSeXUd5_","executionInfo":{"status":"ok","timestamp":1710874942144,"user_tz":240,"elapsed":25484,"user":{"displayName":"Yujia Wu","userId":"06636721410996064596"}},"outputId":"312085e7-9467-4405-bf6b-3b66eb160dfa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloaded ECOSTRESS_L2_LSTE_32159_012_20240308T125756_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32154_001_20240308T044928_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32139_004_20240307T053804_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32093_004_20240304T062514_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32078_004_20240303T071319_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32052_007_20240301T152143_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32037_015_20240229T160915_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32032_003_20240229T080014_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32017_004_20240228T084843_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_32017_003_20240228T084751_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31991_014_20240226T165559_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31971_003_20240225T093440_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31956_004_20240224T102239_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31930_015_20240222T183142_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31915_016_20240221T192025_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31895_003_20240220T115954_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31869_012_20240218T200912_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31869_011_20240218T200820_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31849_002_20240217T124925_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31849_001_20240217T124833_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31808_013_20240214T214550_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31788_005_20240213T142538_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31788_004_20240213T142446_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31747_014_20240210T232233_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31747_013_20240210T232141_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31727_006_20240209T160207_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31727_005_20240209T160115_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31712_005_20240208T164916_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31686_012_20240207T005643_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31666_007_20240205T173624_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31605_006_20240201T191208_0601_03.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31605_005_20240201T191116_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31564_013_20240130T040714_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31544_006_20240128T204625_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31544_005_20240128T204533_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31483_006_20240124T222131_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31422_005_20240120T235720_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31361_005_20240117T013228_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31320_014_20240114T102817_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31254_001_20240110T035554_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31239_002_20240109T044418_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31198_006_20240106T134148_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31193_002_20240106T053349_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31193_001_20240106T053257_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31178_001_20240105T062135_0601_02.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31152_003_20240103T143036_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31152_002_20240103T142944_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31132_001_20240102T071024_0601_01.1.jpg\n","Downloaded ECOSTRESS_L2_LSTE_31117_001_20240101T075834_0601_02.1.jpg\n","All images downloaded and saved to L2_LST_images\n"]}]},{"cell_type":"code","source":["def load_images_from_folder(folder):\n","    images = []\n","    for filename in os.listdir(folder):\n","        img = cv2.imread(os.path.join(folder,filename))\n","        if img is not None:\n","            images.append(img)\n","    return images\n","images = load_images_from_folder(output_folder)\n","\n","resized_images = []\n","for img in images:\n","    resized_img = cv2.resize(img, (128, 128))\n","    resized_images.append(resized_img)\n","\n","numpy_images = np.array(resized_images)\n","\n","# Convert numpy array to PyTorch tensor\n","tensor_images = torch.from_numpy(numpy_images)\n","\n","print(\"Shape of tensor_images:\", tensor_images.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sov-PM5zVWku","executionInfo":{"status":"ok","timestamp":1710875741360,"user_tz":240,"elapsed":948,"user":{"displayName":"Yujia Wu","userId":"06636721410996064596"}},"outputId":"0a8170bb-a1b9-4c4c-e2be-5f451552ad88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor_images: torch.Size([49, 128, 128, 3])\n"]}]},{"cell_type":"code","source":["# Batch size: 4, Channels: 1 (for grayscale images, or it could be 3 for RGB if the data is like that), height: 32, width: 32\n","class CRNN(nn.Module):\n","    def __init__(self):\n","        super(CRNN, self).__init__()\n","\n","        # CNN layers\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3))\n","        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n","        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n","        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        # Flatten layer\n","        self.flatten = nn.Flatten()\n","\n","        # LSTM layer\n","        # Assuming the flattened output is considered as sequ, batch_first=True)\n","ential data for LSTM\n","        self.lstm = nn.LSTM(input_size=1024, hidden_size=1024\n","        # Dense layer\n","        self.fc = nn.Linear(1024, 512)\n","        self.fc1 = nn.Linear(512, 256)\n","        self.fc2 = nn.Linear(256, 1) # output size is the product of height and width of the target map\n","\n","    def forward(self, x):\n","        # Pass data through convolutional layers\n","        x = self.pool1(torch.relu(self.conv1(x)))\n","        x = self.pool2(torch.relu(self.conv2(x)))\n","        x = self.pool3(torch.relu(self.conv3(x)))\n","\n","        # Flatten the output for the LSTM\n","        x = self.flatten(x)\n","\n","        # Reshape data for LSTM layer (batch_size, seq_len, features)\n","        x = x.view(4, -1, 256)\n","\n","        # LSTM layer expects input as (batch, seq, feature)\n","        lstm_out, (hn, cn) = self.lstm(x)\n","\n","        # Only take the output of the last time step (assuming we only predict 1 future step)\n","        # If we want to predict more steps, we should modify this part accordingly\n","        last_time_step = lstm_out[:, -1, :]\n","\n","        # Pass the output of the last time step through the dense layer to get predictions\n","        x = self.fc(last_time_step)\n","        x = F.relu(x)\n","        x = self.fc1(x)\n","        x =F.relu(x)\n","        x = self.fc2(x)\n","\n","        # Reshape the output to the shape of the target map\n","        # x = x.view(4, 36, 36)\n","\n","        return x\n","\n","# Create the CRNN model\n","crnn = CRNN()\n","\n","# Print the model summary\n","print(crnn)\n"],"metadata":{"id":"x1XS5iiMWPXL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710875889345,"user_tz":240,"elapsed":355,"user":{"displayName":"Yujia Wu","userId":"06636721410996064596"}},"outputId":"3bd43145-c9c4-42cf-c65c-c72d185028e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CRNN(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","  (pool3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (lstm): LSTM(1024, 1024, batch_first=True)\n","  (fc): Linear(in_features=1024, out_features=512, bias=True)\n","  (fc1): Linear(in_features=512, out_features=256, bias=True)\n","  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["images = tensor_images.permute(0,3,2,1)\n","# Resize the images to 32x32 and reorder dimensions to [batch_size, channels, height, width]\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),  # Convert numpy images to PIL Image to use torchvision transforms\n","    transforms.Resize((32, 32)),  # Resize image to 32x32\n","    transforms.ToTensor()  # Convert PIL Image to Tensor and reorder dimensions to [C, H, W]\n","])\n","\n","images_transformed = torch.stack([transform(img) for img in images])\n","\n","# Since your dataset size might not be a multiple of your batch size,\n","# let's truncate the dataset to the largest multiple of 4 for simplicity.\n","batch_size = 4\n","num_batches = len(images_transformed) // batch_size\n","images_transformed = images_transformed[:num_batches * batch_size]\n","\n","# Create DataLoader\n","dataset = TensorDataset(images_transformed, y)\n","\n","# Calculate sizes for splitting\n","test_size = int(len(dataset) * 0.2)  # Define the proportion for the test set, e.g., 20%\n","train_size = len(dataset) - test_size  # The rest will be used for the training set\n","\n","# Split the dataset\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Create DataLoaders for both datasets\n","batch_size = 4  # Adjust based on your requirements\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","\n","# Create the CRNN model\n","crnn = CRNN()\n","\n","crnn.train()\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(crnn.parameters(), lr=0.001)\n","\n","# Number of epochs (iterations over the entire training dataset)\n","epochs = 10\n","\n","# Training loop\n","for epoch in tqdm(range(epochs)):\n","    running_loss = 0.0\n","    for data, label in train_loader:\n","\n","        inputs = data[0]\n","        inputs = inputs.float()  # Ensure inputs are float\n","        label = label.float().unsqueeze(1)\n","        optimizer.zero_grad()\n","        outputs = crnn(inputs)\n","        loss = criterion(outputs, label)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:    # Print every 10 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n","            running_loss = 0.0\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2FEBuTWYN3-","executionInfo":{"status":"ok","timestamp":1710880358601,"user_tz":240,"elapsed":7196,"user":{"displayName":"Yujia Wu","userId":"06636721410996064596"}},"outputId":"ccec88b3-4791-46e7-9960-e9c84a9b1a3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:06<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Finished Training\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["crnn.eval()\n","with torch.no_grad():\n","    test_loss = 0.0\n","    for data, label in test_loader:\n","        inputs = data[0]\n","        outputs = crnn(inputs)\n","        loss = criterion(outputs, label)\n","        test_loss += loss.item()\n","\n","    avg_test_loss = test_loss / len(test_loader)\n","    print(f'Average test loss: {avg_test_loss:.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MI262pFRMlmR","executionInfo":{"status":"ok","timestamp":1710880371728,"user_tz":240,"elapsed":308,"user":{"displayName":"Yujia Wu","userId":"06636721410996064596"}},"outputId":"5b573946-e891-45bb-c99c-6c44028712aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test loss: 2.913\n"]}]},{"cell_type":"code","source":["# Redefine the CRNN model with GRU instead of LSTM\n","\n","class CRNN_GRU(nn.Module):\n","    def __init__(self):\n","        super(CRNN_GRU, self).__init__()\n","\n","        # CNN layers\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3))\n","        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n","        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3))\n","        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","\n","        # Flatten layer\n","        self.flatten = nn.Flatten()\n","\n","        # GRU layer\n","        self.gru = nn.GRU(input_size=3072, hidden_size=1024, batch_first=True)\n","\n","        # Dense layer\n","        self.fc = nn.Linear(1024, 36*62) # output size is the product of height and width of the target map\n","\n","    def forward(self, x):\n","        # Pass data through convolutional layers\n","        x = self.pool1(torch.relu(self.conv1(x)))\n","        x = self.pool2(torch.relu(self.conv2(x)))\n","        x = self.pool3(torch.relu(self.conv3(x)))\n","\n","        # Flatten the output for the GRU\n","        x = self.flatten(x)\n","\n","        # Reshape data for GRU layer (batch_size, seq_len, features)\n","        x = x.view(4, -1, 3072)\n","\n","        # GRU layer expects input as (batch, seq, feature)\n","        gru_out, hn = self.gru(x)\n","\n","        # Only take the output of the last time step (assuming we only predict 1 future step)\n","        last_time_step = gru_out[:, -1, :]\n","\n","        # Pass the output of the last time step through the dense layer to get predictions\n","        x = self.fc(last_time_step)\n","\n","        # Reshape the output to the shape of the target map\n","        x = x.view(4, 36, 62)\n","\n","        return x\n","\n","# Create the CRNN model with GRU\n","crnn_gru = CRNN_GRU()\n","\n","# Print the model summary\n","print(crnn_gru)\n"],"metadata":{"id":"ctaAB8N8Wr7m"},"execution_count":null,"outputs":[]}]}